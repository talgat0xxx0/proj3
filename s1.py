!pip install transformers datasets evaluate sacrebleu bert-score sentencepiece
pip install rouge_score
!huggingface-cli login --token=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
pip install sacrebleu
pip install bert_score
pip install evaluate
pip install -U bitsandbytes
# -*- coding: utf-8 -*-vot67

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_from_disk, load_dataset  # <--- –î–æ–±–∞–≤–∏–ª load_dataset
import evaluate
import pandas as pd
import re
from peft import PeftModel

# ===== –ü–£–¢–ò =====
VAL_DATA = "/content/gdrive/MyDrive/trns/gpt_dataset"
VAL_PATH = "/content/gdrive/MyDrive/tengri_test.jsonl" # <--- –ò—Å–ø—Ä–∞–≤–∏–ª –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
BASE_MODEL = "google/gemma-3-4b-it" # –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –º–æ–¥–µ–ª—å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç (–æ–±—ã—á–Ω–æ gemma-2-9b-it)
LORA_PATH  = "talgatzh/gemma-lora-merged0405zzz1"
SAVE_CSV   = "/content/gdrive/MyDrive/evaluation_results.csv"

# 1. –ó–ê–ì–†–£–ó–ö–ê –î–ê–¢–ê–°–ï–¢–ê
print(f"‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑: {VAL_PATH}")
# –ó–∞–≥—Ä—É–∂–∞–µ–º jsonl, –±–µ—Ä–µ–º —Å–ø–ª–∏—Ç train –∏ –≤—ã–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤—ã–µ 50 –ø—Ä–∏–º–µ—Ä–æ–≤
val_dataset = load_dataset("json", data_files=VAL_PATH)["train"].select(range(500))#val_dataset = load_dataset("json", data_files=VAL_PATH)["train"].select(range(50))

# 2. –ó–ê–ì–†–£–ó–ö–ê –¢–û–ö–ï–ù–ò–ó–ï–†–ê
print(f"‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–µ—Ä–∞: {BASE_MODEL}")
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# 3. –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò
print(f"‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {BASE_MODEL}")
model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    device_map="auto",
    torch_dtype=torch.bfloat16,
    load_in_4bit=True
)

print(f"‚è≥ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ LoRA: {LORA_PATH}")
#model = PeftModel.from_pretrained(model, LORA_PATH)
#model.eval()
print("üî• –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")

# ================================================================

def clean_text(text: str) -> str:
    text = re.sub(r"[\n\r]+", " ", text)
    text = re.sub(r"\s{2,}", " ", text)
    return text.strip()

def build_prompt(doc: str):
    messages = [
        {
            "role": "user",
            "content": (
                "–ú”ô—Ç—ñ–Ω–Ω—ñ“£ –º–∞–∑–º“±–Ω—ã–Ω —Ç–æ–ª—ã“õ –∂”ô–Ω–µ –µ–≥–∂–µ–π-—Ç–µ–≥–∂–µ–π–ª—ñ —Å–∏–ø–∞—Ç—Ç–∞–ø –±–µ—Ä. "
                "–ú–∞“ì—ã–Ω–∞–Ω—ã ”©–∑–≥–µ—Ä—Ç–ø–µ, —Å”©–π–ª–µ–º–¥–µ—Ä–¥—ñ –º“Ø–º–∫—ñ–Ω–¥—ñ–≥—ñ–Ω—à–µ –±–∞—Å—Ç–∞–ø“õ—ã —Ç“Ø—Ä—ñ–Ω–¥–µ —Å–∞“õ—Ç–∞.\n\n"
                f"{doc.strip()}\n\n–¢–æ–ª—ã“õ –º–∞–∑–º“±–Ω—ã:"
            )
        }
    ]

    prompt_text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    enc = tokenizer(
        prompt_text,
        return_tensors="pt",
        truncation=True,
        max_length=2048
    )

    return {k: v.to(model.device) for k, v in enc.items()}

# ===== –ù–ê–°–¢–†–û–ô–ö–ò –ì–ï–ù–ï–†–ê–¶–ò–ò =====
gen_cfg = dict(
    max_new_tokens=512,   # <--- –£–≤–µ–ª–∏—á–∏–ª –¥–ª—è "–ø–æ–ª–Ω–æ–≥–æ" –æ–ø–∏—Å–∞–Ω–∏—è
    min_new_tokens=20,
    temperature=0.5,
    do_sample=True,
    repetition_penalty=1.1, # –ú–æ–∂–Ω–æ –ø–æ—Å—Ç–∞–≤–∏—Ç—å 1.1, –µ—Å–ª–∏ –±—É–¥—É—Ç –ø–æ–≤—Ç–æ—Ä—ã
    eos_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id
)

# ================================================================

preds, refs, docs = [], [], []
print("üöÄ –°—Ç–∞—Ä—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏...")

for idx, example in enumerate(val_dataset):
    # –í–ù–ò–ú–ê–ù–ò–ï: –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –≤ jsonl –∫–ª—é—á–∏ –∏–º–µ–Ω–Ω–æ "text" –∏ "title"
    doc = clean_text(example["text"])
    ref = example["title"].strip()

    batch = build_prompt(doc)

    with torch.no_grad():
        output = model.generate(
            input_ids=batch["input_ids"],
            attention_mask=batch["attention_mask"],
            **gen_cfg
        )

    # === –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –ù–ê–ß–ò–ù–ê–ï–¢–°–Ø –ó–î–ï–°–¨ ===

    # 1. –°—á–∏—Ç–∞–µ–º –¥–ª–∏–Ω—É –≤—Ö–æ–¥–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –≤ —Ç–æ–∫–µ–Ω–∞—Ö
    input_len = batch["input_ids"].shape[1]

    # 2. –û—Ç—Ä–µ–∑–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã, –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ
    generated_ids = output[0][input_len:]

    # 3. –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã
    # skip_special_tokens=True –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É–±–µ—Ä–µ—Ç <eos>, <pad> –∏ –ø—Ä–æ—á–∏–µ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã
    decoded = tokenizer.decode(generated_ids, skip_special_tokens=True)

    # –û—á–∏—Å—Ç–∫–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ (—Ç–≤–æ—è —Ñ—É–Ω–∫—Ü–∏—è)
    generated = clean_text(decoded)

    # === –ö–û–ù–ï–¶ –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø ===

    preds.append(generated)
    refs.append(ref)
    docs.append(doc)

    if (idx + 1) % 10 == 0:
        print(f"‚úî {idx+1}/{len(val_dataset)}")

print("‚úî –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –°—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏...")

# ===== –ú–ï–¢–†–ò–ö–ò =====
rouge = evaluate.load("rouge")
scores = rouge.compute(predictions=preds, references=refs, use_stemmer=True)

bert = evaluate.load("bertscore").compute(
    predictions=preds,
    references=refs,
    lang="kk"
)

P = sum(bert["precision"]) / len(bert["precision"])
R = sum(bert["recall"]) / len(bert["recall"])
F1 = sum(bert["f1"]) / len(bert["f1"])

print("\nüìä ROUGE:")
for k, v in scores.items():
    print(f"{k}: {v:.4f}")

chrf = evaluate.load("chrf")
chrf_scores = chrf.compute(predictions=preds, references=refs, word_order=2)

print("\nüî§ chrF:")
for k, v in chrf_scores.items():
    print(f"{k}: {v:.4f}")

print("\nü§ñ BERTScore:")
print(f"P: {P:.4f}\nR: {R:.4f}\nF1: {F1:.4f}")

# –í—ã–≤–æ–¥ –ø–µ—Ä–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
for i in range(min(3, len(docs))):
    print(f"\nüîπ Example {i+1}")
    print(f"üìù Document (start): {docs[i][:100]}...")
    print(f"‚úÖ Reference: {refs[i]}")
    print(f"üß† Generated: {preds[i]}")

# ===== –°–û–•–†–ê–ù–ï–ù–ò–ï =====
pd.DataFrame({
    "document": docs,
    "reference": refs,
    "generated": preds
}).to_csv(SAVE_CSV, index=False)

print("\n‚úÖ Results saved:", SAVE_CSV)